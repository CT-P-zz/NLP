{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Authors: Catarina Pereira (caperei)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as od\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import  word_tokenize\n",
    "#https://github.com/Jupyter-contrib/jupyter_nbextensions_configurator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting only 4 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'soc.religion.christian',\n",
    "              'comp.graphics', 'sci.med']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch_20newsgroups skelearn public dataset\n",
    "twenty_train = fetch_20newsgroups(subset='train',\n",
    "    categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "twenty_test = fetch_20newsgroups(subset='test',\n",
    "    categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=twenty_train.target\n",
    "y_test=twenty_test.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from string import punctuation\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user_defined_stop_words = ['st','rd','hong','kong'] \n",
    "\n",
    "i = nltk.corpus.stopwords.words('english')\n",
    "j = list(punctuation) #+ user_defined_stop_words\n",
    "\n",
    "stopwords = set(i).union(j)\n",
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From: sd345@city.ac.uk (Michael Collier)\\nSubject: Converting images to HP LaserJet III?\\nNntp-Posting-Host: hampton\\nOrganization: The City University\\nLines: 14\\n\\nDoes anyone know of a good way (standard PC application/PD utility) to\\nconvert tif/img/tga files into LaserJet III format.  We would also like to\\ndo the same, converting to HPGL (HP plotter) files.\\n\\nPlease email any response.\\n\\nIs this the correct group?\\n\\nThanks in advance.  Michael.\\n-- \\nMichael Collier (Programmer)                 The Computer Unit,\\nEmail: M.P.Collier@uk.ac.city                The City University,\\nTel: 071 477-8000 x3769                      London,\\nFax: 071 477-8565                            EC1V 0HB.\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From: sd345@city.ac.uk (Michael Collier) Subject: Converting images HP LaserJet III? Nntp-Posting-Host: hampton Organization: The City University Lines: 14 Does anyone know good way (standard PC application/PD utility) convert tif/img/tga files LaserJet III format. We would also like same, converting HPGL (HP plotter) files. Please email response. Is correct group? Thanks advance. Michael. -- Michael Collier (Programmer) The Computer Unit, Email: M.P.Collier@uk.ac.city The City University, Tel: 071 477-8000 x3769 London, Fax: 071 477-8565 EC1V 0HB.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([w for w in twenty_train.data[0].split() if w not in set(stopwords)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From: sd345@city.ac.uk (Michael Collier) Subject: Converting images HP LaserJet III? Nntp-Posting-Host: hampton Organization: The City University Lines: 14 Does anyone know good way (standard PC application/PD utility) convert tif/img/tga files LaserJet III format. We would also like same, converting HPGL (HP plotter) files. Please email response. Is correct group? Thanks advance. Michael. -- Michael Collier (Programmer) The Computer Unit, Email: M.P.Collier@uk.ac.city The City University, Tel: 071 477-8000 x3769 London, Fax: 071 477-8565 EC1V 0HB.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ex=' '.join([w for w in twenty_train.data[0].split() if w not in set(stopwords)])\n",
    "train_ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From: (Michael Collier) Subject: Converting images HP LaserJet III? Nntp-Posting-Host: hampton Organization: The City University Lines: 14 Does anyone know good way (standard PC application/PD utility) convert tif/img/tga files LaserJet III format. We would also like same, converting HPGL (HP plotter) files. Please email response. Is correct group? Thanks advance. Michael. -- Michael Collier (Programmer) The Computer Unit, Email: The City University, Tel: 071 477-8000 x3769 London, Fax: 071 477-8565 EC1V 0HB.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=' '.join([i for i in train_ex.split() if '@' not in i])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove punctuations/digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from michael collier subject converting images hp laserjet iii nntppostinghost hampton organization the city university lines  does anyone know good way standard pc applicationpd utility convert tifimgtga files laserjet iii format we would also like same converting hpgl hp plotter files please email response is correct group thanks advance michael  michael collier programmer the computer unit email the city university tel   x london fax   ecv hb'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = re.sub('[^a-z\\s]', '', x.lower())                  # get rid of noise\n",
    "x = re.sub(\"\\d+\", \" \", x)                              # get rid of numbers\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from michael collier subject convert imag hp laserjet iii nntppostinghost hampton organ the citi univers line doe anyon know good way standard pc applicationpd util convert tifimgtga file laserjet iii format we would also like same convert hpgl hp plotter file pleas email respons is correct group thank advanc michael michael collier programm the comput unit email the citi univers tel x london fax ecv hb'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([porter.stem(w) for w in x.split() ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "    x=' '.join(word_tokenize(x)) #get rid of '/n'\n",
    "    x=' '.join([i for i in x.split() if '@' not in i])#get rid of emails   \n",
    "    x = re.sub(\"\\d+\", \" \", x)# get rid of numbers   \n",
    "    x = re.sub('[^a-z\\s]', '', x.lower())   # get rid of noise (lower and punctuation)\n",
    "    x = ' '.join([w for w in x.split() if w not in set(stopwords)] )# remove stopwords\n",
    "    wx = [porter.stem(w) for w in x.split() ] # stemming\n",
    "    \n",
    "  \n",
    "    return ' '.join(wx)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_train_clean=map(preprocess, twenty_train.data)\n",
    "twenty_train_clean=[x for x in twenty_train_clean]\n",
    "\n",
    "twenty_test_clean=map(preprocess, twenty_test.data)\n",
    "twenty_test_clean=[x for x in twenty_test_clean]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aantalsnijpunten',\n",
       " 'aaoeppaaogovau',\n",
       " 'aap',\n",
       " 'aaplayex',\n",
       " 'aarhu',\n",
       " 'aario',\n",
       " 'aaron',\n",
       " 'aaronc',\n",
       " 'aaronson',\n",
       " 'aatchoo',\n",
       " 'ab',\n",
       " 'abacusconcordiaca',\n",
       " 'abad',\n",
       " 'abandon',\n",
       " 'abat',\n",
       " 'abber',\n",
       " 'abbott',\n",
       " 'abbrevi',\n",
       " 'abc',\n",
       " 'abdomen',\n",
       " 'abdomin',\n",
       " 'abduct',\n",
       " 'abdullah',\n",
       " 'abeit',\n",
       " 'abeka',\n",
       " 'abel',\n",
       " 'aberdeen',\n",
       " 'aberr',\n",
       " 'abhin',\n",
       " 'abhor',\n",
       " 'abhorr',\n",
       " 'abid',\n",
       " 'abideth',\n",
       " 'abigail',\n",
       " 'abil',\n",
       " 'abild',\n",
       " 'abildskov',\n",
       " 'abiliy',\n",
       " 'abington',\n",
       " 'abiogenesi',\n",
       " 'abl',\n",
       " 'ablaz',\n",
       " 'abli',\n",
       " 'abner',\n",
       " 'abnorm',\n",
       " 'abo',\n",
       " 'aboard',\n",
       " 'abod',\n",
       " 'abofi',\n",
       " 'abolish',\n",
       " 'abolit',\n",
       " 'abomb',\n",
       " 'abomin',\n",
       " 'abord',\n",
       " 'abort',\n",
       " 'abortionlaw',\n",
       " 'abou',\n",
       " 'abound',\n",
       " 'abovement',\n",
       " 'abp',\n",
       " 'abpsoft',\n",
       " 'abput',\n",
       " 'abraam',\n",
       " 'abraham',\n",
       " 'abram',\n",
       " 'abrash',\n",
       " 'abraxi',\n",
       " 'abreast',\n",
       " 'abridg',\n",
       " 'abroad',\n",
       " 'abruptli',\n",
       " 'abscenc',\n",
       " 'abscess',\n",
       " 'absenc',\n",
       " 'absent',\n",
       " 'absitin',\n",
       " 'absolabsolcom',\n",
       " 'absolut',\n",
       " 'absolutist',\n",
       " 'absolv',\n",
       " 'absorb',\n",
       " 'absorbt',\n",
       " 'absorpt',\n",
       " 'abstact',\n",
       " 'abstain',\n",
       " 'abstin',\n",
       " 'abstinenceeduc',\n",
       " 'abstinencerel',\n",
       " 'abstinencn',\n",
       " 'abstract',\n",
       " 'abstractdescript',\n",
       " 'abstractsoftcom',\n",
       " 'absurd',\n",
       " 'absurdum',\n",
       " 'abund',\n",
       " 'abundantli',\n",
       " 'abus',\n",
       " 'ac',\n",
       " 'acad',\n",
       " 'academ',\n",
       " 'academi',\n",
       " 'academia',\n",
       " 'acadia',\n",
       " 'acapscsmcgillca',\n",
       " 'acc',\n",
       " 'acceler',\n",
       " 'accent',\n",
       " 'accentu',\n",
       " 'accept',\n",
       " 'acces',\n",
       " 'access',\n",
       " 'accessdigexcom',\n",
       " 'accessdigexnet',\n",
       " 'accessori',\n",
       " 'accid',\n",
       " 'accident',\n",
       " 'accidentpron',\n",
       " 'acclimat',\n",
       " 'acclimitaz',\n",
       " 'accom',\n",
       " 'accommod',\n",
       " 'accomod',\n",
       " 'accompani',\n",
       " 'accomplish',\n",
       " 'accor',\n",
       " 'accord',\n",
       " 'accordingli',\n",
       " 'accost',\n",
       " 'account',\n",
       " 'accpakex',\n",
       " 'accpet',\n",
       " 'accredit',\n",
       " 'accrodingli',\n",
       " 'accru',\n",
       " 'accuart',\n",
       " 'accuat',\n",
       " 'accucorp',\n",
       " 'accufont',\n",
       " 'accukey',\n",
       " 'accumul',\n",
       " 'accupunctur',\n",
       " 'accupuntunc',\n",
       " 'accur',\n",
       " 'accuraci',\n",
       " 'accus',\n",
       " 'accustom',\n",
       " 'accutan',\n",
       " 'ace',\n",
       " 'aceeelblgov',\n",
       " 'acegr',\n",
       " 'aceh',\n",
       " 'aceituna',\n",
       " 'acenetauburnedu',\n",
       " 'acept',\n",
       " 'acet',\n",
       " 'acetominophen',\n",
       " 'acf',\n",
       " 'ach',\n",
       " 'acheiv',\n",
       " 'achesback',\n",
       " 'achiev',\n",
       " 'achim',\n",
       " 'achiv',\n",
       " 'achs',\n",
       " 'aci',\n",
       " 'acid',\n",
       " 'acidophil',\n",
       " 'acidophili',\n",
       " 'acidophiliu',\n",
       " 'acidophilu',\n",
       " 'ack',\n",
       " 'acknosledg',\n",
       " 'acknowled',\n",
       " 'acknowledg',\n",
       " 'aclimat',\n",
       " 'acm',\n",
       " 'acmegennz',\n",
       " 'acmsiggraph',\n",
       " 'acn',\n",
       " 'acoop',\n",
       " 'acord',\n",
       " 'acorn',\n",
       " 'acosta',\n",
       " 'acoust',\n",
       " 'acoustiquemusiqu',\n",
       " 'acpubdukeedu',\n",
       " 'acquaint',\n",
       " 'acquaintac',\n",
       " 'acquir',\n",
       " 'acquisit',\n",
       " 'acquitt',\n",
       " 'acrid',\n",
       " 'acrifr',\n",
       " 'acronym',\n",
       " 'across',\n",
       " 'acsbuedu',\n",
       " 'acsccom',\n",
       " 'acscpsmsuedu',\n",
       " 'acsokstateedu',\n",
       " 'acsubuffaloedu',\n",
       " 'acsuc',\n",
       " 'act',\n",
       " 'actaulli',\n",
       " 'actg',\n",
       " 'action',\n",
       " 'actionguid',\n",
       " 'activ',\n",
       " 'activa',\n",
       " 'activist',\n",
       " 'activitiest',\n",
       " 'acton',\n",
       " 'actor',\n",
       " 'actrix',\n",
       " 'actrixgennz',\n",
       " 'actual',\n",
       " 'actuallay',\n",
       " 'actuel',\n",
       " 'actup',\n",
       " 'actupnew',\n",
       " 'acupunctur',\n",
       " 'acupuncturist',\n",
       " 'acurcio',\n",
       " 'acut',\n",
       " 'acutan',\n",
       " 'acutechron',\n",
       " 'acuuraci',\n",
       " 'acvax',\n",
       " 'acyclovir',\n",
       " 'ad',\n",
       " 'adaba',\n",
       " 'adaci',\n",
       " 'adaclabscom',\n",
       " 'adag',\n",
       " 'adagiopanasoniccom',\n",
       " 'adam',\n",
       " 'adamantli',\n",
       " 'adamev',\n",
       " 'adamllmitedu',\n",
       " 'adamski',\n",
       " 'adapt',\n",
       " 'adaptiov',\n",
       " 'adaptor',\n",
       " 'aday',\n",
       " 'adc',\n",
       " 'add',\n",
       " 'adda',\n",
       " 'addendum',\n",
       " 'adder',\n",
       " 'addict',\n",
       " 'addin',\n",
       " 'addisonwesley',\n",
       " 'addit',\n",
       " 'additionscorrectionssuggest',\n",
       " 'addon',\n",
       " 'address',\n",
       " 'addressphon',\n",
       " 'adelaid',\n",
       " 'adelphiitdadelaideeduau',\n",
       " 'adelphoi',\n",
       " 'aden',\n",
       " 'adeno',\n",
       " 'adenoma',\n",
       " 'adeospold',\n",
       " 'adept',\n",
       " 'adequ',\n",
       " 'adher',\n",
       " 'adhes',\n",
       " 'adiminist',\n",
       " 'adipos',\n",
       " 'adit',\n",
       " 'adjac',\n",
       " 'adjud',\n",
       " 'adjust',\n",
       " 'adjuv',\n",
       " 'adkin',\n",
       " 'adl',\n",
       " 'adler',\n",
       " 'adm',\n",
       " 'admin',\n",
       " 'administ',\n",
       " 'administr',\n",
       " 'admir',\n",
       " 'admirador',\n",
       " 'admiss',\n",
       " 'admit',\n",
       " 'admittedli',\n",
       " 'admonish',\n",
       " 'admonit',\n",
       " 'adnet',\n",
       " 'adob',\n",
       " 'adobecom',\n",
       " 'adolesc',\n",
       " 'adolf',\n",
       " 'adolphu',\n",
       " 'adoni',\n",
       " 'adopt',\n",
       " 'adoption',\n",
       " 'ador',\n",
       " 'adorn',\n",
       " 'adp',\n",
       " 'adpro',\n",
       " 'adren',\n",
       " 'adrenalgland',\n",
       " 'adress',\n",
       " 'adrg',\n",
       " 'adrgzip',\n",
       " 'adriaansen',\n",
       " 'adrian',\n",
       " 'adriana',\n",
       " 'adshead',\n",
       " 'adspsuborg',\n",
       " 'adu',\n",
       " 'adul',\n",
       " 'adult',\n",
       " 'adulter',\n",
       " 'adulteri',\n",
       " 'adultreport',\n",
       " 'adultress',\n",
       " 'advan',\n",
       " 'advanc',\n",
       " 'advantag',\n",
       " 'advent',\n",
       " 'adventist',\n",
       " 'adventur',\n",
       " 'advers',\n",
       " 'adversari',\n",
       " 'advert',\n",
       " 'advertis',\n",
       " 'advic',\n",
       " 'adviceexperi',\n",
       " 'advis',\n",
       " 'advisor',\n",
       " 'advisori',\n",
       " 'advoc',\n",
       " 'advocaci',\n",
       " 'adwright',\n",
       " 'adwyer',\n",
       " 'ae',\n",
       " 'aegdstogovau',\n",
       " 'aerial',\n",
       " 'aerob',\n",
       " 'aerodynam',\n",
       " 'aerosol',\n",
       " 'aerospac',\n",
       " 'aesthet',\n",
       " 'aeta',\n",
       " 'af',\n",
       " 'afanh',\n",
       " 'afar',\n",
       " 'afari',\n",
       " 'affair',\n",
       " 'affect',\n",
       " 'affection',\n",
       " 'affff',\n",
       " 'afffff',\n",
       " 'affiar',\n",
       " 'affili',\n",
       " 'affin',\n",
       " 'affirm',\n",
       " 'afflict',\n",
       " 'afford',\n",
       " 'affraid',\n",
       " 'afghan',\n",
       " 'afil',\n",
       " 'afix',\n",
       " 'aflatoxin',\n",
       " 'afoul',\n",
       " 'afoxx',\n",
       " 'afp',\n",
       " 'afraid',\n",
       " 'africa',\n",
       " 'african',\n",
       " 'africanamerican',\n",
       " 'afrin',\n",
       " 'afrocanadiantop',\n",
       " 'afront',\n",
       " 'afteral',\n",
       " 'aftereffect',\n",
       " 'afterlif',\n",
       " 'afternoon',\n",
       " 'afterthefact',\n",
       " 'afterward',\n",
       " 'ag',\n",
       " 'aga',\n",
       " 'agabu',\n",
       " 'agaist',\n",
       " 'agar',\n",
       " 'agarenginumichedu',\n",
       " 'agateberkeleyedu',\n",
       " 'agb',\n",
       " 'age',\n",
       " 'agenc',\n",
       " 'agenda',\n",
       " 'agent',\n",
       " 'agernow',\n",
       " 'agglutin',\n",
       " 'aggrav',\n",
       " 'aggres',\n",
       " 'aggress',\n",
       " 'aggriv',\n",
       " 'aghast',\n",
       " 'agiacalo',\n",
       " 'agian',\n",
       " 'agianst',\n",
       " 'agilmet',\n",
       " 'aginst',\n",
       " 'agnost',\n",
       " 'agnostic',\n",
       " 'ago',\n",
       " 'agoin',\n",
       " 'agon',\n",
       " 'agoni',\n",
       " 'agonizingli',\n",
       " 'agoraphob',\n",
       " 'agoraphobia',\n",
       " 'agre',\n",
       " 'agree',\n",
       " 'agreeabl',\n",
       " 'agreement',\n",
       " 'agress',\n",
       " 'agricultur',\n",
       " 'agrino',\n",
       " 'ah',\n",
       " 'aha',\n",
       " 'ahab',\n",
       " 'ahaz',\n",
       " 'ahd',\n",
       " 'ahead',\n",
       " 'ahem',\n",
       " 'ahh',\n",
       " 'ahhh',\n",
       " 'ahm',\n",
       " 'ahmad',\n",
       " 'ahmadi',\n",
       " 'ahmadiyya',\n",
       " 'ahmadiyyaislam',\n",
       " 'ahpcrc',\n",
       " 'ahpcrcumnedu',\n",
       " 'ahuja',\n",
       " 'ai',\n",
       " 'aic',\n",
       " 'aid',\n",
       " 'aidler',\n",
       " 'aidshiv',\n",
       " 'aidsit',\n",
       " 'aidsrel',\n",
       " 'aiken',\n",
       " 'ail',\n",
       " 'ailingzhufreeman',\n",
       " 'ailment',\n",
       " 'aim',\n",
       " 'aimitedu',\n",
       " 'aimlacom',\n",
       " 'aingrcom',\n",
       " 'ainn',\n",
       " 'aio',\n",
       " 'aiojscnasagov',\n",
       " 'aip',\n",
       " 'aipd',\n",
       " 'aipsmail',\n",
       " 'air',\n",
       " 'airborn',\n",
       " 'aircool',\n",
       " 'aircraft',\n",
       " 'aircrew',\n",
       " 'airhead',\n",
       " 'airlin',\n",
       " 'airplan',\n",
       " 'airplaneread',\n",
       " 'airport',\n",
       " 'airway',\n",
       " 'aisha',\n",
       " 'aisricom',\n",
       " 'aisun',\n",
       " 'aiugaedu',\n",
       " 'aix',\n",
       " 'aixrpiedu',\n",
       " 'aj',\n",
       " 'ajackson',\n",
       " 'ajay',\n",
       " 'ajinn',\n",
       " 'ajr',\n",
       " 'ajrxzxnvroblfaqfsbwpmhepi',\n",
       " 'ak',\n",
       " 'aka',\n",
       " 'akademi',\n",
       " 'akbar',\n",
       " 'akgua',\n",
       " 'akguaattcom',\n",
       " 'akh',\n",
       " 'akiernan',\n",
       " 'akin',\n",
       " 'akita',\n",
       " 'akron',\n",
       " 'akway',\n",
       " 'al',\n",
       " 'ala',\n",
       " 'alabama',\n",
       " 'alaikum',\n",
       " 'alamo',\n",
       " 'alan',\n",
       " 'alanb',\n",
       " 'alarm',\n",
       " 'alaska',\n",
       " 'alaskaedu',\n",
       " 'alaskan',\n",
       " 'alastair',\n",
       " 'alazhar',\n",
       " 'alba',\n",
       " 'albani',\n",
       " 'albatross',\n",
       " 'albedo',\n",
       " 'albeit',\n",
       " 'albert',\n",
       " 'alberta',\n",
       " 'albican',\n",
       " 'albnyvmsbitnet',\n",
       " 'albrecht',\n",
       " 'albright',\n",
       " 'albuquerqu',\n",
       " 'alcfh',\n",
       " 'alchemi',\n",
       " 'alchohol',\n",
       " 'alchol',\n",
       " 'alcohol',\n",
       " 'alcorconcordiaca',\n",
       " 'ald',\n",
       " 'aldhfnakronohu',\n",
       " 'aldou',\n",
       " 'aldri',\n",
       " 'aldridg',\n",
       " 'aldridgec',\n",
       " 'aldu',\n",
       " 'ale',\n",
       " 'aleahi',\n",
       " 'alejandro',\n",
       " 'alert',\n",
       " 'alessandro',\n",
       " 'alex',\n",
       " 'alexand',\n",
       " 'alexanderjam',\n",
       " 'alexandria',\n",
       " 'alexandrian',\n",
       " 'alexeevich',\n",
       " 'alexi',\n",
       " 'alexialisuiucedu',\n",
       " 'alfonso',\n",
       " 'alforja',\n",
       " 'alfr',\n",
       " 'alfredcarletonca',\n",
       " 'alfredccscarletonca',\n",
       " 'alg',\n",
       " 'algebra',\n",
       " 'algeria',\n",
       " 'algier',\n",
       " 'algiorithm',\n",
       " 'algirithm',\n",
       " 'algo',\n",
       " 'algocount',\n",
       " 'algorhtym',\n",
       " 'algorithm',\n",
       " 'algorithmhidden',\n",
       " 'algorithmscod',\n",
       " 'algoritm',\n",
       " 'ali',\n",
       " 'alia',\n",
       " 'alias',\n",
       " 'alic',\n",
       " 'aliceattcom',\n",
       " 'aliceb',\n",
       " 'aliceuucp',\n",
       " 'alicia',\n",
       " 'alien',\n",
       " 'aliesterccutexasedu',\n",
       " 'align',\n",
       " 'alik',\n",
       " 'alink',\n",
       " 'alittl',\n",
       " 'aliv',\n",
       " 'alkalin',\n",
       " 'all',\n",
       " 'alla',\n",
       " 'alladin',\n",
       " 'allah',\n",
       " 'allan',\n",
       " 'allaround',\n",
       " 'allcap',\n",
       " 'alleg',\n",
       " 'allegedli',\n",
       " 'allegedu',\n",
       " 'allegheni',\n",
       " 'allegi',\n",
       " 'allegor',\n",
       " 'allegori',\n",
       " 'allegraattcom',\n",
       " 'allegro',\n",
       " 'alleiat',\n",
       " 'allen',\n",
       " 'allerg',\n",
       " 'allergi',\n",
       " 'allergicsinu',\n",
       " 'allergist',\n",
       " 'alleri',\n",
       " 'allevi',\n",
       " 'allgraphicsjpeg',\n",
       " 'alli',\n",
       " 'allianc',\n",
       " 'alliant',\n",
       " 'allign',\n",
       " 'allison',\n",
       " 'allknow',\n",
       " 'allnatur',\n",
       " 'alloc',\n",
       " 'allout',\n",
       " 'allov',\n",
       " 'allow',\n",
       " 'allperfect',\n",
       " 'allpervad',\n",
       " 'allpow',\n",
       " 'allright',\n",
       " 'allsphinxtarz',\n",
       " 'alltough',\n",
       " 'allud',\n",
       " 'allwis',\n",
       " 'alm',\n",
       " 'almac',\n",
       " 'almaccouk',\n",
       " 'almadenibmcom',\n",
       " 'almah',\n",
       " 'almanac',\n",
       " 'almight',\n",
       " 'almighti',\n",
       " 'almodovar',\n",
       " 'almond',\n",
       " 'almost',\n",
       " 'alo',\n",
       " 'aload',\n",
       " 'alogirhtm',\n",
       " 'alogorythmn',\n",
       " 'alogrithm',\n",
       " 'alon',\n",
       " 'along',\n",
       " 'alongsid',\n",
       " 'alopez',\n",
       " 'alot',\n",
       " 'aloud',\n",
       " 'aloy',\n",
       " 'aloysiu',\n",
       " 'alp',\n",
       " 'alpha',\n",
       " 'alphabet',\n",
       " 'alphachannel',\n",
       " 'alphanumer',\n",
       " 'alphashap',\n",
       " 'alqanawi',\n",
       " 'alread',\n",
       " 'alreadi',\n",
       " 'alreadyalt',\n",
       " 'alreadycod',\n",
       " 'alright',\n",
       " 'also',\n",
       " 'alsoinvolv',\n",
       " 'alt',\n",
       " 'alta',\n",
       " 'altaircsustanedu',\n",
       " 'altalienvisitor',\n",
       " 'altansw',\n",
       " 'altaohcom',\n",
       " 'altar',\n",
       " 'altariq',\n",
       " 'altath',\n",
       " 'altatheismarchivenam',\n",
       " 'altatheismmoder',\n",
       " 'altatheist',\n",
       " 'altatheisthard',\n",
       " 'altathiesm',\n",
       " 'altautoth',\n",
       " 'altbestofinternet',\n",
       " 'altbinariespicturesmisc',\n",
       " 'altbinariespicturesutil',\n",
       " 'altbinariespixutil',\n",
       " 'altdrug',\n",
       " 'alter',\n",
       " 'altern',\n",
       " 'altertum',\n",
       " 'altfanbrotherj',\n",
       " 'altfandanquayl',\n",
       " 'altfandouglasadam',\n",
       " 'altflam',\n",
       " 'altflameseanryan',\n",
       " 'altgraphicspixutil',\n",
       " 'although',\n",
       " 'altimetri',\n",
       " 'altitud',\n",
       " 'altman',\n",
       " 'altmedcf',\n",
       " 'altmessian',\n",
       " 'alto',\n",
       " 'altogeth',\n",
       " 'altoona',\n",
       " 'altprintscreen',\n",
       " 'altpsychoact',\n",
       " 'altpsychologyperson',\n",
       " 'altraytrac',\n",
       " 'altreligion',\n",
       " 'altrocknrollmet',\n",
       " 'altruist',\n",
       " 'altsciastroaip',\n",
       " 'altsciphysicsnewtheori',\n",
       " 'altsmartdrug',\n",
       " 'altsourc',\n",
       " 'altsourcew',\n",
       " 'alttasteless',\n",
       " 'altwed',\n",
       " 'altx',\n",
       " 'alu',\n",
       " 'aludrauscedu',\n",
       " 'alum',\n",
       " 'aluminum',\n",
       " 'alumni',\n",
       " 'alumnu',\n",
       " 'alunat',\n",
       " 'alv',\n",
       " 'alvin',\n",
       " 'alvusersrequest',\n",
       " 'alway',\n",
       " 'alwnihgov',\n",
       " 'alyc',\n",
       " 'alyosha',\n",
       " 'alza',\n",
       " 'ama',\n",
       " 'amalgam',\n",
       " 'amann',\n",
       " 'amantadin',\n",
       " 'amateur',\n",
       " 'amaz',\n",
       " 'amazingli',\n",
       " 'amazonian',\n",
       " 'amb',\n",
       " 'amber',\n",
       " 'ambercecerarmymil',\n",
       " 'amberssdcsdharriscom',\n",
       " 'amberucsindianaedu',\n",
       " 'ambient',\n",
       " 'ambigu',\n",
       " 'ambit',\n",
       " 'ambros',\n",
       " 'ambul',\n",
       " 'ame',\n",
       " 'amelior',\n",
       " 'amen',\n",
       " 'amend',\n",
       " 'amer',\n",
       " 'america',\n",
       " 'american',\n",
       " 'amertumeufrinfop',\n",
       " 'amesarcnasagov',\n",
       " 'amgad',\n",
       " 'amherst',\n",
       " 'ami',\n",
       " 'amic',\n",
       " 'amidst',\n",
       " 'amiga',\n",
       " 'amigado',\n",
       " 'amigaexec',\n",
       " 'amigagfx',\n",
       " 'amigagfxconv',\n",
       " 'amigagfxedithamlab',\n",
       " 'amigagfxshowviewtek',\n",
       " 'amigajpegv',\n",
       " 'amigalynx',\n",
       " 'amigan',\n",
       " 'amigao',\n",
       " 'amigaphysikunizhch',\n",
       " 'amigapixmisc',\n",
       " 'amigaunix',\n",
       " 'amigo',\n",
       " 'amil',\n",
       " 'aminet',\n",
       " 'amino',\n",
       " 'amitriptylin',\n",
       " 'amjad',\n",
       " 'ammair',\n",
       " 'ammunit',\n",
       " 'amn',\n",
       " 'amnesia',\n",
       " 'amnesti',\n",
       " 'amniocentesi',\n",
       " 'amo',\n",
       " 'amok',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amor',\n",
       " 'amount',\n",
       " 'ampakzenetdeccom',\n",
       " 'ampex',\n",
       " 'ampexcom',\n",
       " 'amphibia',\n",
       " 'amphiboli',\n",
       " 'amphotericin',\n",
       " 'ampl',\n",
       " 'amplif',\n",
       " 'amplifi',\n",
       " 'ampute',\n",
       " 'amr',\n",
       " 'amro',\n",
       " 'amsterdam',\n",
       " 'amstrad',\n",
       " 'amt',\n",
       " 'amtadvic',\n",
       " 'amtrak',\n",
       " 'amugwaichimeduacjp',\n",
       " 'amus',\n",
       " 'ana',\n",
       " 'anabol',\n",
       " 'anaerob',\n",
       " 'anafranil',\n",
       " 'anagram',\n",
       " 'anahericssoncom',\n",
       " 'anal',\n",
       " 'analges',\n",
       " 'analgesicantiinflammatori',\n",
       " 'analog',\n",
       " 'analogu',\n",
       " 'analretent',\n",
       " 'analys',\n",
       " 'analysi',\n",
       " 'analyst',\n",
       " 'analystprogramm',\n",
       " 'analyt',\n",
       " 'analyz',\n",
       " 'anaphylact',\n",
       " 'anarch',\n",
       " 'anarchi',\n",
       " 'anarchist',\n",
       " 'anasaz',\n",
       " 'anasazi',\n",
       " 'anasazicom',\n",
       " 'anatolia',\n",
       " 'anatom',\n",
       " 'anatomi',\n",
       " 'anatumsmededu',\n",
       " 'anaysi',\n",
       " 'anba',\n",
       " 'anc',\n",
       " 'ancestor',\n",
       " 'ancestr',\n",
       " 'ancestri',\n",
       " 'anchor',\n",
       " 'anchorag',\n",
       " 'anchoresdsgicom',\n",
       " 'ancient',\n",
       " 'ancyra',\n",
       " 'and',\n",
       " 'andactuallyfulfillgodswil',\n",
       " 'andersom',\n",
       " 'anderson',\n",
       " 'andersongabriel',\n",
       " 'andi',\n",
       " 'andor',\n",
       " 'andov',\n",
       " 'andr',\n",
       " 'andrea',\n",
       " 'andreasa',\n",
       " 'andrebeck',\n",
       " 'andrew',\n",
       " 'andrewcmuedu',\n",
       " 'andrey',\n",
       " 'androlog',\n",
       " 'andromeda',\n",
       " 'andromedarutgersedu',\n",
       " 'andru',\n",
       " 'andso',\n",
       " 'andtbacka',\n",
       " 'andybgsuedu',\n",
       " 'andyercmsstateedu',\n",
       " 'anecdot',\n",
       " 'anecedot',\n",
       " 'anectdot',\n",
       " 'anello',\n",
       " 'anemia',\n",
       " 'anest',\n",
       " 'anesthesia',\n",
       " 'anesthesiolog',\n",
       " 'anesthet',\n",
       " 'aneurysm',\n",
       " 'anew',\n",
       " 'angel',\n",
       " 'angelino',\n",
       " 'anger',\n",
       " 'angl',\n",
       " 'anglesw',\n",
       " 'anglican',\n",
       " 'anglophil',\n",
       " 'anglosaxon',\n",
       " 'angri',\n",
       " 'angrili',\n",
       " 'anguish',\n",
       " 'angul',\n",
       " 'angusmiorg',\n",
       " 'anh',\n",
       " 'ani',\n",
       " 'anim',\n",
       " 'animateworld',\n",
       " 'animationvideo',\n",
       " 'animatorsth',\n",
       " 'animt',\n",
       " 'aninnm',\n",
       " 'aniruddha',\n",
       " 'ankerberg',\n",
       " 'ankl',\n",
       " 'anl',\n",
       " 'ann',\n",
       " 'anna',\n",
       " 'annal',\n",
       " 'annala',\n",
       " 'anne',\n",
       " 'annemari',\n",
       " 'annex',\n",
       " 'annia',\n",
       " 'annick',\n",
       " 'annihil',\n",
       " 'annim',\n",
       " 'anno',\n",
       " 'annoint',\n",
       " 'annot',\n",
       " 'annoth',\n",
       " 'announc',\n",
       " 'announcementof',\n",
       " 'annoy',\n",
       " 'annual',\n",
       " 'annul',\n",
       " 'annunci',\n",
       " 'anoint',\n",
       " 'anon',\n",
       " 'anonym',\n",
       " 'anonyomu',\n",
       " 'anorect',\n",
       " 'anoth',\n",
       " 'ansaid',\n",
       " 'anscestori',\n",
       " 'anselm',\n",
       " 'ansi',\n",
       " 'ansiaiim',\n",
       " 'ansselin',\n",
       " 'answer',\n",
       " 'ant',\n",
       " 'antagonist',\n",
       " 'antarct',\n",
       " 'antecd',\n",
       " 'anteced',\n",
       " 'anterior',\n",
       " 'antero',\n",
       " 'anthem',\n",
       " 'antholog',\n",
       " 'anthoni',\n",
       " 'anthropo',\n",
       " 'anthropolog',\n",
       " 'anthropologist',\n",
       " 'anthropomorphis',\n",
       " 'anthroposophi',\n",
       " 'anthrpomorph',\n",
       " 'anti',\n",
       " 'antialias',\n",
       " 'antiatheist',\n",
       " 'antibacteri',\n",
       " 'antibelief',\n",
       " 'antibigot',\n",
       " 'antibiot',\n",
       " 'antibodi',\n",
       " 'anticanc',\n",
       " 'anticanceraid',\n",
       " 'antichiropract',\n",
       " 'antichrist',\n",
       " 'antichristian',\n",
       " 'anticip',\n",
       " 'anticonvuls',\n",
       " 'anticrack',\n",
       " 'anticryptographi',\n",
       " 'antidepress',\n",
       " 'antidepressanttyp',\n",
       " 'antidot',\n",
       " 'antiecolog',\n",
       " 'antieduc',\n",
       " 'antifreez',\n",
       " 'antifung',\n",
       " 'antigen',\n",
       " 'antihatr',\n",
       " 'antihistamin',\n",
       " 'antihistimin',\n",
       " 'antihistor',\n",
       " 'antiinflamitori',\n",
       " 'antiinflammatori',\n",
       " 'antiintellectu',\n",
       " 'antiislam',\n",
       " 'antileg',\n",
       " 'antimicrobi',\n",
       " 'antimoslem',\n",
       " 'antimsg',\n",
       " 'antineoplaston',\n",
       " ...]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'brownian': 2764,\n",
       " 'wane': 24745,\n",
       " 'casefind': 3232,\n",
       " 'rehabilit': 18958,\n",
       " 'imnsho': 10623,\n",
       " 'needlesus': 15088,\n",
       " 'astrolog': 1410,\n",
       " 'tragic': 23156,\n",
       " 'io': 11207,\n",
       " 'righthand': 19346,\n",
       " 'timores': 22935,\n",
       " 'wavelength': 24814,\n",
       " 'turnpik': 23422,\n",
       " 'geometr': 8754,\n",
       " 'atheismlog': 1448,\n",
       " 'strike': 21802,\n",
       " 'jb': 11481,\n",
       " 'ranichemyaleedu': 18611,\n",
       " 'sociolog': 21100,\n",
       " 'mixcommixcomcom': 14345,\n",
       " 'kilti': 12095,\n",
       " 'altfandouglasadam': 698,\n",
       " 'bypass': 2962,\n",
       " 'wotanmedicinerochesteredu': 25310,\n",
       " 'multicent': 14750,\n",
       " 'ceticsuncedu': 3464,\n",
       " 'grassulist': 9171,\n",
       " 'cel': 3404,\n",
       " 'mvusadobecom': 14844,\n",
       " 'talktyp': 22355,\n",
       " 'johncha': 11669,\n",
       " 'ersatzfolk': 7220,\n",
       " 'subtl': 21924,\n",
       " 'wrong': 25357,\n",
       " 'pregnanc': 17646,\n",
       " 'snortsingestsshootsup': 21065,\n",
       " 'petri': 16938,\n",
       " 'mathemat': 13623,\n",
       " 'dingebr': 5935,\n",
       " 'archivecisohiostateedu': 1219,\n",
       " 'territori': 22617,\n",
       " 'rj': 19398,\n",
       " 'drawingpictur': 6379,\n",
       " 'saten': 19888,\n",
       " 'distributor': 6124,\n",
       " 'wardel': 24756,\n",
       " 'miya': 14348,\n",
       " 'problemat': 17810,\n",
       " 'lombardi': 13009,\n",
       " 'mindcontrol': 14194,\n",
       " 'paschal': 16611,\n",
       " 'boardschip': 2452,\n",
       " 'northeast': 15632,\n",
       " 'jkellett': 11622,\n",
       " 'vera': 24345,\n",
       " 'idl': 10465,\n",
       " 'twintwinsuncom': 23451,\n",
       " 'underserv': 23745,\n",
       " 'luoma': 13188,\n",
       " 'inexact': 10825,\n",
       " 'eastren': 6635,\n",
       " 'empath': 6943,\n",
       " 'precomput': 17615,\n",
       " 'lybrand': 13210,\n",
       " 'stoney': 21727,\n",
       " 'jensen': 11544,\n",
       " 'truk': 23334,\n",
       " 'sq': 21462,\n",
       " 'heniz': 9783,\n",
       " 'thingi': 22774,\n",
       " 'petervanderveen': 16932,\n",
       " 'realworld': 18767,\n",
       " 'propanolol': 17904,\n",
       " 'tmpsm': 22984,\n",
       " 'inject': 10940,\n",
       " 'altsourcew': 722,\n",
       " 'kmcvay': 12155,\n",
       " 'toungu': 23107,\n",
       " 'bisonacsubuffaloedu': 2293,\n",
       " 'grassx': 9173,\n",
       " 'exportlcsmitedu': 7511,\n",
       " 'freehand': 8337,\n",
       " 'sic': 20710,\n",
       " 'slave': 20928,\n",
       " 'direct': 5958,\n",
       " 'natur': 14994,\n",
       " 'iscariot': 11302,\n",
       " 'cynthia': 5204,\n",
       " 'questionnair': 18455,\n",
       " 'egg': 6776,\n",
       " 'acquaint': 197,\n",
       " 'zealou': 25704,\n",
       " 'weingart': 24885,\n",
       " 'salcido': 19787,\n",
       " 'ieow': 10479,\n",
       " 'lighter': 12796,\n",
       " 'sff': 20472,\n",
       " 'anesthesiolog': 894,\n",
       " 'dcl': 5406,\n",
       " 'wm': 25222,\n",
       " 'maddox': 13304,\n",
       " 'mizan': 14350,\n",
       " 'peta': 16923,\n",
       " 'mania': 13453,\n",
       " 'vmasmsueduext': 24559,\n",
       " 'succ': 21937,\n",
       " 'prophylaxi': 17914,\n",
       " 'partnership': 16598,\n",
       " 'gasguzzl': 8618,\n",
       " 'straw': 21771,\n",
       " 'acton': 221,\n",
       " 'rsd': 19603,\n",
       " 'convolut': 4584,\n",
       " 'deceiv': 5466,\n",
       " 'jeremi': 11548,\n",
       " 'bet': 2122,\n",
       " 'fujita': 8471,\n",
       " 'amongst': 802,\n",
       " 'erythropoiten': 7229,\n",
       " 'chrusher': 3767,\n",
       " 'job': 11650,\n",
       " 'formic': 8214,\n",
       " 'carwil': 3225,\n",
       " 'manip': 13458,\n",
       " 'softlab': 21126,\n",
       " 'nephrol': 15136,\n",
       " 'daalo': 5221,\n",
       " 'lambadaoituncedu': 12391,\n",
       " 'hayward': 9625,\n",
       " 'alon': 651,\n",
       " 'treason': 23227,\n",
       " 'maze': 13685,\n",
       " 'counterfeit': 4729,\n",
       " 'agarenginumichedu': 398,\n",
       " 'store': 21735,\n",
       " 'atheismfaqintrotxt': 1443,\n",
       " 'scream': 20132,\n",
       " 'glandular': 8895,\n",
       " 'poenginumichedu': 17315,\n",
       " 'histogramcontrast': 9975,\n",
       " 'commemor': 4201,\n",
       " 'flat': 8014,\n",
       " 'rusti': 19687,\n",
       " 'nasogastr': 14976,\n",
       " 'us': 24092,\n",
       " 'matt': 13646,\n",
       " 'math': 13619,\n",
       " 'toffler': 23009,\n",
       " 'chb': 3584,\n",
       " 'bronx': 2748,\n",
       " 'rigeldfrfnasagov': 19338,\n",
       " 'cinecait': 3810,\n",
       " 'cleanli': 3919,\n",
       " 'formed': 8211,\n",
       " 'capac': 3116,\n",
       " 'suffici': 21965,\n",
       " 'bia': 2169,\n",
       " 'hurryup': 10301,\n",
       " 'deciplin': 5481,\n",
       " 'odwyer': 15914,\n",
       " 'impetu': 10643,\n",
       " 'iniqu': 10934,\n",
       " 'blaze': 2364,\n",
       " 'cixcompulinkcouk': 3867,\n",
       " 'everi': 7365,\n",
       " 'dfci': 5790,\n",
       " 'outwardli': 16306,\n",
       " 'christian': 3734,\n",
       " 'game': 8576,\n",
       " 'quak': 18415,\n",
       " 'peripheri': 16863,\n",
       " 'johng': 11671,\n",
       " 'yk': 25621,\n",
       " 'pubbryanwpcjpegwecjzip': 18081,\n",
       " 'oscar': 16209,\n",
       " 'exclud': 7430,\n",
       " 'vmsmaccwiscedu': 24568,\n",
       " 'lap': 12432,\n",
       " 'nucleu': 15742,\n",
       " 'johnshepardsonesh': 11678,\n",
       " 'stone': 21725,\n",
       " 'monash': 14451,\n",
       " 'kelli': 11973,\n",
       " 'modelnam': 14402,\n",
       " 'marku': 13538,\n",
       " 'ciprservermghharvardedu': 3816,\n",
       " 'unoccupi': 23916,\n",
       " 'alto': 708,\n",
       " 'poke': 17342,\n",
       " 'johnf': 11670,\n",
       " 'wick': 25067,\n",
       " 'hutt': 10317,\n",
       " 'skim': 20898,\n",
       " 'precondom': 17616,\n",
       " 'anasaz': 846,\n",
       " 'meaningless': 13804,\n",
       " 'dup': 6530,\n",
       " 'getworldtyp': 8794,\n",
       " 'kentcommuucp': 11998,\n",
       " 'osag': 16205,\n",
       " 'hrubin': 10217,\n",
       " 'ramada': 18583,\n",
       " 'bala': 1754,\n",
       " 'minfminfvubacb': 14202,\n",
       " 'storagetek': 21734,\n",
       " 'phenylketoneurea': 16989,\n",
       " 'mission': 14310,\n",
       " 'sarcoidosi': 19867,\n",
       " 'dryden': 6434,\n",
       " 'csnorg': 5017,\n",
       " 'revealeth': 19236,\n",
       " 'faculteit': 7608,\n",
       " 'headtrip': 9660,\n",
       " 'cernapocernch': 3448,\n",
       " 'miner': 14200,\n",
       " 'prefect': 17640,\n",
       " 'leger': 12610,\n",
       " 'posapanaorgau': 17464,\n",
       " 'crystallographi': 4963,\n",
       " 'multiplay': 14761,\n",
       " 'epilepsi': 7135,\n",
       " 'hissef': 9971,\n",
       " 'toe': 23008,\n",
       " 'necess': 15072,\n",
       " 'scaveng': 19973,\n",
       " 'rbnsn': 18698,\n",
       " 'kit': 12129,\n",
       " 'distend': 6109,\n",
       " 'irretriev': 11281,\n",
       " 'christu': 3753,\n",
       " 'thumb': 22864,\n",
       " 'hpvclcvcdhpcom': 10205,\n",
       " 'incap': 10707,\n",
       " 'methodist': 14014,\n",
       " 'siggraph': 20738,\n",
       " 'giaecccmonasheduau': 8819,\n",
       " 'app': 1103,\n",
       " 'fornic': 8220,\n",
       " 'statur': 21596,\n",
       " 'povray': 17540,\n",
       " 'epson': 7151,\n",
       " 'emailbonycom': 6900,\n",
       " 'nibbi': 15321,\n",
       " 'log': 12992,\n",
       " 'randomli': 18607,\n",
       " 'dolphin': 6260,\n",
       " 'hpqmocbsqfhpcom': 10201,\n",
       " 'cling': 3963,\n",
       " 'andromedarutgersedu': 881,\n",
       " 'ommit': 16022,\n",
       " 'nois': 15463,\n",
       " 'atw': 1520,\n",
       " 'ahpcrcumnedu': 448,\n",
       " 'sleeman': 20938,\n",
       " 'winmark': 25156,\n",
       " 'superced': 22048,\n",
       " 'quit': 18478,\n",
       " 'dpi': 6353,\n",
       " 'registercompat': 18944,\n",
       " 'distinctli': 6112,\n",
       " 'proventil': 17966,\n",
       " 'clibsucc': 3949,\n",
       " 'pseudolymphoma': 18006,\n",
       " 'synergyst': 22262,\n",
       " 'melt': 13898,\n",
       " 'groningen': 9260,\n",
       " 'buddihsm': 2837,\n",
       " 'benbest': 2053,\n",
       " 'cheesi': 3601,\n",
       " 'none': 15512,\n",
       " 'sharewar': 20537,\n",
       " 'simmon': 20790,\n",
       " 'wixercactusorg': 25211,\n",
       " 'vibrat': 24427,\n",
       " 'distinguish': 6113,\n",
       " 'indic': 10774,\n",
       " 'sustain': 22146,\n",
       " 'doubleblind': 6321,\n",
       " 'zamenhofcsriceedu': 25689,\n",
       " 'narr': 14959,\n",
       " 'cmuedu': 4031,\n",
       " 'ianf': 10393,\n",
       " 'leong': 12660,\n",
       " 'coninu': 4432,\n",
       " 'hadith': 9408,\n",
       " 'mdauthtmcedu': 13784,\n",
       " 'acenetauburnedu': 163,\n",
       " 'dubin': 6485,\n",
       " 'highli': 9902,\n",
       " 'homolog': 10095,\n",
       " 'lucki': 13150,\n",
       " 'unquestioningli': 23938,\n",
       " 'csserasu': 5032,\n",
       " 'telectron': 22524,\n",
       " 'macedonian': 13254,\n",
       " 'palat': 16448,\n",
       " 'anoth': 949,\n",
       " 'turkish': 23415,\n",
       " 'venom': 24338,\n",
       " 'csusthk': 5059,\n",
       " 'scheifler': 19994,\n",
       " 'sird': 20849,\n",
       " 'occur': 15882,\n",
       " 'blupe': 2432,\n",
       " 'migrat': 14136,\n",
       " 'breast': 2669,\n",
       " 'sort': 21226,\n",
       " 'ceremoni': 3444,\n",
       " 'cambridg': 3058,\n",
       " 'sabbat': 19721,\n",
       " 'vancanes': 24244,\n",
       " 'chay': 3583,\n",
       " 'scour': 20120,\n",
       " 'boulder': 2581,\n",
       " 'fidei': 7858,\n",
       " 'loi': 13006,\n",
       " 'embodi': 6917,\n",
       " 'sensori': 20379,\n",
       " 'alqanawi': 665,\n",
       " 'sodom': 21118,\n",
       " 'function': 8496,\n",
       " 'multius': 14775,\n",
       " 'poetri': 17319,\n",
       " 'weed': 24868,\n",
       " 'universum': 23865,\n",
       " 'harrisji': 9562,\n",
       " 'calcium': 3015,\n",
       " 'bain': 1745,\n",
       " 'carvel': 3223,\n",
       " 'pubdkbtrac': 18089,\n",
       " 'arisen': 1254,\n",
       " 'defin': 5542,\n",
       " 'imagin': 10572,\n",
       " 'assign': 1383,\n",
       " 'exchang': 7423,\n",
       " 'mccarter': 13708,\n",
       " 'sharpli': 20544,\n",
       " 'intrigu': 11168,\n",
       " 'moralist': 14524,\n",
       " 'staunch': 21599,\n",
       " 'fransi': 8308,\n",
       " 'nephi': 15135,\n",
       " 'snazzi': 21042,\n",
       " 'apparantli': 1106,\n",
       " 'xm': 25472,\n",
       " 'fineman': 7921,\n",
       " 'probe': 17807,\n",
       " 'omnisci': 16028,\n",
       " 'addict': 259,\n",
       " 'mvv': 14845,\n",
       " 'ga': 8539,\n",
       " 'alyc': 738,\n",
       " 'wkuvx': 25217,\n",
       " 'track': 23138,\n",
       " 'sarajevo': 19860,\n",
       " 'qbbmp': 18320,\n",
       " 'devast': 5762,\n",
       " 'teslanjitedu': 22625,\n",
       " 'mizvertrieb': 14351,\n",
       " 'iiuibno': 10514,\n",
       " 'organ': 16162,\n",
       " 'abraham': 74,\n",
       " 'slouch': 20974,\n",
       " 'spaceag': 21262,\n",
       " 'datri': 5356,\n",
       " 'find': 7915,\n",
       " 'dad': 5229,\n",
       " 'kellog': 11974,\n",
       " 'sexism': 20461,\n",
       " 'rohvm': 19487,\n",
       " 'icrf': 10437,\n",
       " 'sieburg': 20722,\n",
       " 'billw': 2229,\n",
       " 'kasif': 11932,\n",
       " 'magicult': 13339,\n",
       " 'schmel': 20009,\n",
       " 'tzotzilspanish': 23511,\n",
       " 'vp': 24642,\n",
       " 'underneath': 23739,\n",
       " 'increasingli': 10746,\n",
       " 'govern': 9070,\n",
       " 'licenc': 12763,\n",
       " 'rape': 18622,\n",
       " 'amn': 795,\n",
       " 'supersav': 22070,\n",
       " 'teapot': 22471,\n",
       " 'fossil': 8241,\n",
       " 'cabl': 2975,\n",
       " 'eindhoven': 6803,\n",
       " 'slice': 20944,\n",
       " 'welldocu': 24915,\n",
       " 'davidwheatonedu': 5379,\n",
       " 'bore': 2544,\n",
       " 'yoke': 25632,\n",
       " 'melittin': 13889,\n",
       " 'quantit': 18427,\n",
       " 'chamberlain': 3510,\n",
       " 'record': 18850,\n",
       " 'ellus': 6876,\n",
       " 'affirm': 369,\n",
       " 'rohmhaascom': 19485,\n",
       " 'yeah': 25590,\n",
       " 'arff': 1232,\n",
       " 'instantli': 11031,\n",
       " 'authorit': 1573,\n",
       " 'bauwesen': 1900,\n",
       " 'mccool': 13715,\n",
       " 'straighten': 21750,\n",
       " 'populum': 17435,\n",
       " 'counterproduct': 4732,\n",
       " 'aberr': 38,\n",
       " 'xenophobia': 25431,\n",
       " 'caboomcbmdeccom': 2977,\n",
       " 'eicn': 6797,\n",
       " 'equat': 7162,\n",
       " 'grandfath': 9119,\n",
       " 'juan': 11782,\n",
       " 'cannabl': 3096,\n",
       " 'worship': 25302,\n",
       " 'whatsoev': 24988,\n",
       " 'shous': 20675,\n",
       " 'mid': 14104,\n",
       " 'robin': 19440,\n",
       " 'flourish': 8058,\n",
       " 'regener': 18934,\n",
       " 'poitier': 17341,\n",
       " 'preemptiv': 17635,\n",
       " 'helmet': 9747,\n",
       " 'phobiatrauma': 17028,\n",
       " 'sawdust': 19927,\n",
       " 'ivywpiedu': 11405,\n",
       " 'technician': 22479,\n",
       " 'woltr': 25236,\n",
       " 'accordingli': 137,\n",
       " 'trek': 23240,\n",
       " 'structuralist': 21827,\n",
       " 'tasteless': 22419,\n",
       " 'hrdb': 10212,\n",
       " 'littin': 12914,\n",
       " 'intellectioncom': 11063,\n",
       " 'unw': 24021,\n",
       " 'cml': 4025,\n",
       " 'vistapro': 24528,\n",
       " 'iii': 10507,\n",
       " 'runni': 19669,\n",
       " 'moselem': 14569,\n",
       " 'sweep': 22189,\n",
       " 'fraction': 8276,\n",
       " 'coughran': 4707,\n",
       " 'loss': 13059,\n",
       " 'topolog': 23071,\n",
       " 'proof': 17897,\n",
       " 'parsley': 16578,\n",
       " 'nynex': 15796,\n",
       " 'instead': 11033,\n",
       " 'microsoft': 14092,\n",
       " 'anugula': 1044,\n",
       " 'ap': 1067,\n",
       " 'pooltabl': 17413,\n",
       " 'mathufledu': 13637,\n",
       " 'melewitt': 13884,\n",
       " 'bandag': 1783,\n",
       " 'semianarch': 20345,\n",
       " 'titu': 22964,\n",
       " 'shoppa': 20652,\n",
       " 'fortun': 8235,\n",
       " 'timmon': 22933,\n",
       " 'greatest': 9195,\n",
       " 'jhl': 11592,\n",
       " 'ummm': 23637,\n",
       " 'ishikawa': 11312,\n",
       " 'mikroeetuberlind': 14143,\n",
       " 'newstekcom': 15280,\n",
       " 'alic': 578,\n",
       " 'highspatialfrequ': 9914,\n",
       " 'dearest': 5439,\n",
       " 'bskendig': 2800,\n",
       " 'btw': 2813,\n",
       " 'neron': 15142,\n",
       " 'welbon': 24896,\n",
       " 'welch': 24897,\n",
       " 'caucasu': 3281,\n",
       " 'atheismfaqresourcetxt': 1445,\n",
       " 'hot': 10159,\n",
       " 'ece': 6661,\n",
       " 'austin': 1557,\n",
       " 'expenditur': 7478,\n",
       " 'lccsdsdlocuscom': 12544,\n",
       " 'novak': 15683,\n",
       " 'wplymouth': 25328,\n",
       " 'chilicclehighedu': 3658,\n",
       " 'wainwright': 24706,\n",
       " 'nanci': 14938,\n",
       " 'kiki': 12080,\n",
       " 'vaxen': 24288,\n",
       " 'daze': 5393,\n",
       " 'dug': 6499,\n",
       " 'divniti': 6151,\n",
       " 'priestridden': 17755,\n",
       " 'adaci': 241,\n",
       " 'stalin': 21527,\n",
       " 'planner': 17214,\n",
       " 'bed': 1974,\n",
       " 'guidanc': 9323,\n",
       " 'dobbin': 6218,\n",
       " 'extend': 7523,\n",
       " 'soori': 21209,\n",
       " 'hardcopi': 9522,\n",
       " 'style': 21861,\n",
       " 'sibbald': 20708,\n",
       " 'prohibit': 17868,\n",
       " 'shope': 20651,\n",
       " 'clinton': 3967,\n",
       " 'ftpmacarchitec': 8442,\n",
       " 'geneva': 8718,\n",
       " 'jsn': 11776,\n",
       " 'repost': 19105,\n",
       " 'richardson': 19310,\n",
       " 'compgraphicsresearch': 4277,\n",
       " 'liturg': 12918,\n",
       " 'gentli': 8738,\n",
       " 'colostomi': 4161,\n",
       " 'select': 20285,\n",
       " 'worldserv': 25289,\n",
       " 'joshua': 11716,\n",
       " 'joshuaf': 11718,\n",
       " 'snippet': 21056,\n",
       " 'abund': 105,\n",
       " 'soninlaw': 21201,\n",
       " 'bournemouth': 2590,\n",
       " 'jerk': 11551,\n",
       " 'zbr': 25697,\n",
       " 'wrinkl': 25346,\n",
       " 'touchi': 23100,\n",
       " 'esink': 7243,\n",
       " 'omicroncsuncedu': 16017,\n",
       " 'macmolecul': 13275,\n",
       " 'white': 25029,\n",
       " 'inn': 10956,\n",
       " 'overlaid': 16336,\n",
       " 'xview': 25528,\n",
       " 'misrememb': 14304,\n",
       " 'marbl': 13490,\n",
       " 'usatoday': 24098,\n",
       " 'undoubtedli': 23771,\n",
       " 'anaysi': 853,\n",
       " 'wilfr': 25100,\n",
       " 'lmvec': 12959,\n",
       " 'sunset': 22034,\n",
       " 'plenum': 17251,\n",
       " 'thermographi': 22738,\n",
       " 'green': 9202,\n",
       " 'superiorcarletonca': 22061,\n",
       " 'thermomet': 22739,\n",
       " 'tough': 23103,\n",
       " 'duke': 6503,\n",
       " 'unwis': 24027,\n",
       " 'ancyra': 863,\n",
       " 'classifi': 3903,\n",
       " 'juventutem': 11857,\n",
       " 'emit': 6936,\n",
       " 'resourc': 19169,\n",
       " 'colorquant': 4154,\n",
       " 'tzotzil': 23510,\n",
       " 'chapel': 3536,\n",
       " 'moron': 14542,\n",
       " 'hat': 9587,\n",
       " 'tie': 22897,\n",
       " 'heikki': 9715,\n",
       " 'hypoglycemia': 10366,\n",
       " 'stuffpictpixiehqx': 21847,\n",
       " 'tempera': 22548,\n",
       " 'own': 16383,\n",
       " 'netherland': 15166,\n",
       " 'bitftp': 2303,\n",
       " 'doubtless': 6333,\n",
       " 'jen': 11538,\n",
       " 'dusti': 6541,\n",
       " 'configur': 4408,\n",
       " 'rich': 19307,\n",
       " 'claremont': 3888,\n",
       " 'vaxastevenstechedu': 24281,\n",
       " 'mimic': 14187,\n",
       " 'ouchemchemoaklandedu': 16264,\n",
       " 'behind': 2008,\n",
       " 'desktop': 5719,\n",
       " 'go': 8975,\n",
       " 'lyme': 13213,\n",
       " 'msdlmsclockheedcom': 14676,\n",
       " 'onion': 16059,\n",
       " 'transposit': 23201,\n",
       " 'appropri': 1147,\n",
       " 'pantherbear': 16493,\n",
       " 'steak': 21612,\n",
       " 'navyrel': 15006,\n",
       " 'prettier': 17725,\n",
       " 'manasseh': 13431,\n",
       " 'imagfr': 10569,\n",
       " 'dataset': 5352,\n",
       " 'opportunist': 16119,\n",
       " 'shazam': 20558,\n",
       " 'uabdpodpouabedu': 23515,\n",
       " 'pubx': 18208,\n",
       " 'expert': 7486,\n",
       " 'park': 16562,\n",
       " 'scotsmen': 20112,\n",
       " 'unsuspect': 23994,\n",
       " 'randomdigitdi': 18606,\n",
       " 'cywang': 5214,\n",
       " 'pant': 16490,\n",
       " 'msdosgraphicsjpegsrc': 14688,\n",
       " 'banschbach': 1800,\n",
       " 'screwbal': 20140,\n",
       " 'ucsan': 23542,\n",
       " 'behs': 2011,\n",
       " 'foard': 8098,\n",
       " 'postcondit': 17490,\n",
       " 'repons': 19101,\n",
       " 'airborn': 472,\n",
       " 'gtf': 9303,\n",
       " 'sharedmemori': 20535,\n",
       " 'resum': 19197,\n",
       " 'demograph': 5622,\n",
       " 'oklahoma': 15985,\n",
       " 'sourcecod': 21241,\n",
       " 'report': 19102,\n",
       " 'genu': 8739,\n",
       " 'letharg': 12683,\n",
       " 'judgment': 11797,\n",
       " 'isol': 11332,\n",
       " 'vcpicompat': 24297,\n",
       " 'eolacsucfedu': 7112,\n",
       " 'soapbox': 21081,\n",
       " 'lunar': 13176,\n",
       " 'legendakronohu': 12608,\n",
       " 'requiem': 19131,\n",
       " 'vox': 24637,\n",
       " 'vietnam': 24456,\n",
       " 'ogr': 15959,\n",
       " 'haldol': 9434,\n",
       " 'billh': 2222,\n",
       " 'djcoyl': 6160,\n",
       " 'historia': 9980,\n",
       " 'comforatbl': 4192,\n",
       " 'opencsfsuedu': 16094,\n",
       " 'psychiatr': 18031,\n",
       " 'pollster': 17373,\n",
       " 'fitt': 7980,\n",
       " 'theo': 22702,\n",
       " 'razon': 18693,\n",
       " 'niec': 15354,\n",
       " 'aflatoxin': 376,\n",
       " 'hudson': 10249,\n",
       " 'uncommon': 23693,\n",
       " 'aka': 495,\n",
       " 'fuzzi': 8530,\n",
       " 'amigagfxconv': 777,\n",
       " 'monochromat': 14471,\n",
       " 'nyx': 15802,\n",
       " 'disassembl': 5976,\n",
       " 'arperd': 1296,\n",
       " 'woodyapanaorgau': 25257,\n",
       " 'inu': 11184,\n",
       " 'korzybski': 12234,\n",
       " 'beth': 2128,\n",
       " 'pqdor': 17568,\n",
       " 'itll': 11378,\n",
       " 'simul': 20807,\n",
       " 'replay': 19093,\n",
       " 'pig': 17117,\n",
       " 'erl': 7203,\n",
       " 'holliaug': 10069,\n",
       " 'fatwa': 7710,\n",
       " 'elig': 6855,\n",
       " 'slower': 20979,\n",
       " 'disgust': 6031,\n",
       " 'faq': 7675,\n",
       " 'aaoeppaaogovau': 12,\n",
       " 'copernicu': 4605,\n",
       " 'carolin': 3192,\n",
       " 'muhammadi': 14739,\n",
       " 'castaway': 3242,\n",
       " 'libray': 12755,\n",
       " 'phx': 17066,\n",
       " 'lousier': 13075,\n",
       " 'ubiquit': 23526,\n",
       " 'frontier': 8398,\n",
       " 'server': 20429,\n",
       " 'lavang': 12511,\n",
       " 'bako': 1753,\n",
       " 'tactic': 22322,\n",
       " 'spebcg': 21305,\n",
       " 'tsavohkscom': 23350,\n",
       " 'rockvil': 19461,\n",
       " 'tverski': 23428,\n",
       " 'darken': 5309,\n",
       " 'chromosom': 3758,\n",
       " 'fantasyland': 7673,\n",
       " 'dissatisfi': 6093,\n",
       " 'deonstrydom': 5656,\n",
       " 'guldenspoorstraat': 9337,\n",
       " 'dat': 5336,\n",
       " 'denning': 5642,\n",
       " 'clasp': 3898,\n",
       " 'ruberi': 19637,\n",
       " 'tpi': 23126,\n",
       " 'censu': 3420,\n",
       " 'saguaro': 19770,\n",
       " 'coronado': 4663,\n",
       " 'beast': 1951,\n",
       " 'scholorship': 20023,\n",
       " 'subroutin': 21901,\n",
       " 'cold': 4115,\n",
       " 'fastperftrig': 7697,\n",
       " 'utkvxbitnet': 24165,\n",
       " 'rennig': 19064,\n",
       " 'lumpur': 13173,\n",
       " 'hardisk': 9531,\n",
       " 'signal': 20748,\n",
       " 'utkvxutkedu': 24166,\n",
       " 'religiosidad': 19019,\n",
       " 'review': 19244,\n",
       " 'donat': 6276,\n",
       " 'estradiol': 7279,\n",
       " 'reynold': 19265,\n",
       " 'oher': 15964,\n",
       " 'agon': 421,\n",
       " 'probabl': 17803,\n",
       " 'ivanasdsgicom': 11392,\n",
       " 'ttttttt': 23373,\n",
       " 'qem': 18329,\n",
       " 'anchoresdsgicom': 861,\n",
       " 'nervmnerdcufledu': 15145,\n",
       " 'lioness': 12873,\n",
       " 'thrust': 22855,\n",
       " 'pastim': 16637,\n",
       " 'displac': 6072,\n",
       " 'mutual': 14841,\n",
       " 'wak': 24713,\n",
       " 'cardiologydopl': 3159,\n",
       " 'embalm': 6910,\n",
       " 'ousia': 16272,\n",
       " 'dwatson': 6559,\n",
       " 'kaaru': 11881,\n",
       " 'bribe': 2695,\n",
       " 'rmiteduau': 19418,\n",
       " 'audiowork': 1529,\n",
       " 'noncitizen': 15494,\n",
       " 'hodg': 10042,\n",
       " 'skinni': 20904,\n",
       " 'endoscopi': 7008,\n",
       " 'neopaint': 15126,\n",
       " 'lori': 13057,\n",
       " 'medusajplnasagov': 13857,\n",
       " 'hardon': 9533,\n",
       " 'levit': 12704,\n",
       " 'geobench': 8743,\n",
       " 'pass': 16616,\n",
       " 'leap': 12575,\n",
       " 'dishonor': 6036,\n",
       " 'shelter': 20583,\n",
       " 'toughto': 23105,\n",
       " 'ryou': 19710,\n",
       " 'microgram': 14080,\n",
       " 'spoken': 21406,\n",
       " 'entrop': 7085,\n",
       " 'knowabl': 12178,\n",
       " 'muirm': 14742,\n",
       " 'thester': 22751,\n",
       " 'analyz': 841,\n",
       " 'datahanddesc': 5345,\n",
       " 'ineff': 10804,\n",
       " 'insulin': 11050,\n",
       " 'westpac': 24964,\n",
       " 'efw': 6768,\n",
       " 'magisterium': 13342,\n",
       " 'wilaya': 25091,\n",
       " 'freed': 8333,\n",
       " 'isodata': 11330,\n",
       " 'brother': 2757,\n",
       " 'bocher': 2466,\n",
       " 'lruk': 13122,\n",
       " 'pubscipap': 18176,\n",
       " 'hyde': 10328,\n",
       " 'basiliensi': 1867,\n",
       " 'iraqi': 11244,\n",
       " 'jojo': 11685,\n",
       " 'intern': 11106,\n",
       " 'genesi': 8709,\n",
       " 'aplenti': 1082,\n",
       " 'parkin': 16563,\n",
       " 'deamin': 5433,\n",
       " 'petitio': 16935,\n",
       " 'decwrldeccom': 5506,\n",
       " 'tauruscsnpsnavymil': 22426,\n",
       " 'laudabl': 12491,\n",
       " 'bllacberkeleyedu': 2390,\n",
       " 'congeni': 4422,\n",
       " 'jhoskin': 11593,\n",
       " 'practic': 17574,\n",
       " 'eddi': 6697,\n",
       " 'nostalgia': 15647,\n",
       " 'xform': 25440,\n",
       " 'oilhhhkuwait': 15971,\n",
       " 'biotech': 2274,\n",
       " 'geoffrey': 8748,\n",
       " 'coredump': 4638,\n",
       " 'eggstub': 6780,\n",
       " 'compressor': 4308,\n",
       " 'userdevelop': 24125,\n",
       " 'mapgenplotgen': 13483,\n",
       " 'healer': 9662,\n",
       " 'immateri': 10593,\n",
       " 'adriaansen': 317,\n",
       " 'dgrafsi': 5803,\n",
       " 'esuoc': 7282,\n",
       " 'gaul': 8639,\n",
       " 'feet': 7764,\n",
       " 'elli': 6867,\n",
       " 'iview': 11399,\n",
       " 'terrain': 22607,\n",
       " 'duction': 6493,\n",
       " 'graylevel': 9187,\n",
       " 'ehb': 6793,\n",
       " 'islambecaus': 11319,\n",
       " 'clown': 4002,\n",
       " 'mmackey': 14370,\n",
       " 'frameaccur': 8283,\n",
       " 'sport': 21420,\n",
       " 'inmetcambinmetcom': 10955,\n",
       " 'inclus': 10722,\n",
       " 'lindaburi': 12829,\n",
       " 'bldg': 2367,\n",
       " 'dohertyl': 6253,\n",
       " 'imping': 10647,\n",
       " 'degen': 5553,\n",
       " 'vanish': 24254,\n",
       " 'adultreport': 327,\n",
       " 'hoskin': 10150,\n",
       " 'interchang': 11076,\n",
       " 'sketchi': 20890,\n",
       " 'audac': 1524,\n",
       " 'hmo': 10023,\n",
       " 'methodolog': 14015,\n",
       " 'symasussexacuk': 22238,\n",
       " 'handbook': 9476,\n",
       " 'hybrid': 10327,\n",
       " 'cslsonycojp': 5009,\n",
       " 'diphenhydramin': 5950,\n",
       " 'xbm': 25416,\n",
       " 'isometr': 11335,\n",
       " 'jehoshaphat': 11529,\n",
       " 'ya': 25557,\n",
       " 'subjectivist': 21883,\n",
       " 'idiopath': 10461,\n",
       " 'mfbinnhvu': 14031,\n",
       " 'mcpherson': 13766,\n",
       " 'whoever': 25040,\n",
       " 'firstgrad': 7962,\n",
       " 'aidshiv': 454,\n",
       " 'descript': 5700,\n",
       " 'usenetaltatheismlog': 24114,\n",
       " 'havenwat': 9607,\n",
       " 'adag': 243,\n",
       " 'someday': 21175,\n",
       " 'springdal': 21438,\n",
       " 'johnston': 11680,\n",
       " 'lupron': 13190,\n",
       " 'monstrou': 14488,\n",
       " 'qjipo': 18358,\n",
       " 'laped': 12435,\n",
       " 'conundrum': 4567,\n",
       " 'spoil': 21404,\n",
       " 'confession': 4404,\n",
       " 'ionship': 11215,\n",
       " 'clippendal': 3971,\n",
       " 'vocabulari': 24582,\n",
       " 'vertigo': 24387,\n",
       " 'sim': 20781,\n",
       " 'atrov': 1495,\n",
       " 'gilden': 8846,\n",
       " 'concret': 4377,\n",
       " 'profess': 17841,\n",
       " 'testament': 22630,\n",
       " 'tangent': 22379,\n",
       " 'ignor': 10492,\n",
       " 'jpeggifbmp': 11744,\n",
       " 'salem': 19789,\n",
       " 'ghostdsiunimiit': 8811,\n",
       " 'beacaus': 1937,\n",
       " 'mwramedford': 14865,\n",
       " 'doom': 6291,\n",
       " 'ncarucaredu': 15024,\n",
       " 'paster': 16634,\n",
       " 'inargu': 10696,\n",
       " 'deperson': 5662,\n",
       " 'intoller': 11153,\n",
       " 'five': 7984,\n",
       " 'scholarli': 20020,\n",
       " 'padl': 16422,\n",
       " 'satisfac': 19895,\n",
       " 'habit': 9397,\n",
       " 'experienti': 7482,\n",
       " 'malenacr': 13411,\n",
       " 'applicationinterest': 1129,\n",
       " 'stowel': 21747,\n",
       " 'pubsgiclrview': 18179,\n",
       " 'vela': 24324,\n",
       " 'spell': 21347,\n",
       " 'potter': 17528,\n",
       " 'xli': 25467,\n",
       " 'grudg': 9282,\n",
       " 'overtransl': 16366,\n",
       " 'isneed': 11327,\n",
       " 'volvulu': 24613,\n",
       " 'accent': 117,\n",
       " 'priceless': 17747,\n",
       " 'sieg': 20725,\n",
       " 'zazen': 25695,\n",
       " 'general': 8704,\n",
       " 'paramet': 16532,\n",
       " 'aload': 647,\n",
       " 'commer': 4208,\n",
       " 'jude': 11792,\n",
       " 'pictelpictelcom': 17101,\n",
       " 'catheter': 3271,\n",
       " 'ludatlths': 13152,\n",
       " 'twice': 23445,\n",
       " 'bulk': 2865,\n",
       " 'snapstatpurdueedu': 21039,\n",
       " 'morphin': 14545,\n",
       " 'copt': 4624,\n",
       " 'orphan': 16189,\n",
       " 'crimin': 4873,\n",
       " 'cdi': 3381,\n",
       " 'compaq': 4242,\n",
       " 'ubc': 23524,\n",
       " 'ii': 10503,\n",
       " 'errorpron': 7218,\n",
       " 'intra': 11156,\n",
       " 'brockton': 2739,\n",
       " 'uncorrupt': 23708,\n",
       " 'lpi': 13114,\n",
       " 'tesel': 22623,\n",
       " 'calendarremind': 3024,\n",
       " 'discreet': 6010,\n",
       " 'kaspar': 11935,\n",
       " 'exemplari': 7443,\n",
       " 'ny': 15788,\n",
       " 'all': 591,\n",
       " 'papain': 16501,\n",
       " 'spacesuit': 21268,\n",
       " 'nanotech': 14943,\n",
       " 'inhibit': 10927,\n",
       " 'dsnyder': 6447,\n",
       " 'nibley': 15323,\n",
       " 'coal': 4047,\n",
       " 'jman': 11633,\n",
       " 'semialgebra': 20344,\n",
       " 'shipppm': 20626,\n",
       " 'medicin': 13838,\n",
       " 'keplerunhedu': 12001,\n",
       " 'perhap': 16855,\n",
       " 'cellarorg': 3415,\n",
       " 'tommi': 23038,\n",
       " 'pnyyvat': 17300,\n",
       " 'gv': 9366,\n",
       " 'clemson': 3935,\n",
       " 'falconaamrlwpafbafmil': 7638,\n",
       " 'backlash': 1701,\n",
       " 'welldescrib': 24912,\n",
       " 'treefold': 23237,\n",
       " 'ounc': 16269,\n",
       " 'pinnqp': 17151,\n",
       " 'papaci': 16500,\n",
       " 'jkjec': 11626,\n",
       " 'malaysia': 13403,\n",
       " 'atlant': 1479,\n",
       " 'unsound': 23981,\n",
       " 'sorceress': 21217,\n",
       " 'pushkey': 18282,\n",
       " 'bp': 2613,\n",
       " 'lac': 12365,\n",
       " 'abput': 72,\n",
       " 'pain': 16431,\n",
       " 'finlayson': 7938,\n",
       " 'dichotomi': 5854,\n",
       " 'vaselin': 24268,\n",
       " 'von': 24615,\n",
       " 'limit': 12819,\n",
       " 'yee': 25600,\n",
       " 'jkpg': 11629,\n",
       " 'bog': 2478,\n",
       " 'stark': 21565,\n",
       " 'hypercholer': 10344,\n",
       " 'comput': 4332,\n",
       " 'submit': 21895,\n",
       " 'shaz': 20556,\n",
       " 'nottingham': 15676,\n",
       " 'cornflak': 4660,\n",
       " ...}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing rare words\n",
    "count_vect.vocabulary_={k: v for k, v in count_vect.vocabulary_.items() if v >10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25776"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encoder (OHE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (2257, 2257)\n",
      "Wall time: 8.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc.fit(np.array(twenty_train_clean).reshape(-1, 1))\n",
    "X_train_one_hot = enc.transform(np.array(twenty_train_clean).reshape(-1, 1))\n",
    "X_test_one_hot = enc.transform(np.array(twenty_test_clean).reshape(-1, 1))\n",
    "\n",
    "\n",
    "print('shape:',X_train_one_hot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "l = LogisticRegression()\n",
    "r = RandomForestClassifier(n_estimators=25,max_depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 1.3820832429200065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caperei\\AppData\\Local\\Continuum\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\caperei\\AppData\\Local\\Continuum\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "l.fit(X_train_one_hot,y_train)\n",
    "y_pred = l.predict_proba(X_test_one_hot)\n",
    "print('LogisticRegression',log_loss(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier 1.3824453707362796\n"
     ]
    }
   ],
   "source": [
    "r.fit(X_train_one_hot,y_train)\n",
    "y_pred = r.predict_proba(X_test_one_hot)\n",
    "print('RandomForestClassifier',log_loss(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import FeatureHasher\n",
    "\n",
    "h = FeatureHasher(n_features=2257,input_type=\"string\")\n",
    "X_train_hash = h.transform(np.array(twenty_train_clean).reshape(-1, 1))\n",
    "X_test_hash = h.transform(np.array(twenty_test_clean).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4195699252387106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caperei\\AppData\\Local\\Continuum\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\caperei\\AppData\\Local\\Continuum\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "l.fit(X_train_hash,y_train)\n",
    "y_pred = l.predict_proba(X_test_hash)\n",
    "print(log_loss(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.382336249342293\n"
     ]
    }
   ],
   "source": [
    "r.fit(X_train_hash,y_train)\n",
    "y_pred = r.predict_proba(X_test_hash)\n",
    "print(log_loss(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caperei\\AppData\\Local\\Continuum\\anaconda3\\envs\\myenv\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import gensim \n",
    "import logging\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec receives words\n",
    "twenty_train_clean_w2v=list(map(nltk.word_tokenize, twenty_train_clean))\n",
    "twenty_test_clean_w2v=list(map(nltk.word_tokenize, twenty_test_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-10 16:32:21,430 : INFO : collecting all words and their counts\n",
      "2018-10-10 16:32:21,432 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-10-10 16:32:21,565 : INFO : collected 25806 word types from a corpus of 374650 raw words and 2257 sentences\n",
      "2018-10-10 16:32:21,565 : INFO : Loading a fresh vocabulary\n",
      "2018-10-10 16:32:21,634 : INFO : effective_min_count=3 retains 10966 unique words (42% of original 25806, drops 14840)\n",
      "2018-10-10 16:32:21,635 : INFO : effective_min_count=3 leaves 355809 word corpus (94% of original 374650, drops 18841)\n",
      "2018-10-10 16:32:21,695 : INFO : deleting the raw counts dictionary of 25806 items\n",
      "2018-10-10 16:32:21,700 : INFO : sample=0.001 downsamples 23 most-common words\n",
      "2018-10-10 16:32:21,702 : INFO : downsampling leaves estimated 342946 word corpus (96.4% of prior 355809)\n",
      "2018-10-10 16:32:21,780 : INFO : estimated required memory for 10966 words and 200 dimensions: 23028600 bytes\n",
      "2018-10-10 16:32:21,782 : INFO : resetting layer weights\n",
      "2018-10-10 16:32:22,060 : INFO : training model with 4 workers on 10966 vocabulary and 200 features, using sg=1 hs=0 sample=0.001 negative=5 window=6\n",
      "2018-10-10 16:32:23,142 : INFO : EPOCH 1 - PROGRESS: at 45.41% examples, 154660 words/s, in_qsize 6, out_qsize 1\n",
      "2018-10-10 16:32:24,142 : INFO : EPOCH 1 - PROGRESS: at 91.27% examples, 152182 words/s, in_qsize 3, out_qsize 1\n",
      "2018-10-10 16:32:24,144 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-10-10 16:32:24,220 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-10 16:32:24,257 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-10 16:32:24,295 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-10 16:32:24,297 : INFO : EPOCH - 1 : training on 374650 raw words (342943 effective words) took 2.2s, 153831 effective words/s\n",
      "2018-10-10 16:32:25,397 : INFO : EPOCH 2 - PROGRESS: at 41.25% examples, 135864 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 16:32:26,261 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-10-10 16:32:26,299 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-10 16:32:26,345 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-10 16:32:26,358 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-10 16:32:26,360 : INFO : EPOCH - 2 : training on 374650 raw words (343103 effective words) took 2.1s, 167150 effective words/s\n",
      "2018-10-10 16:32:27,369 : INFO : EPOCH 3 - PROGRESS: at 46.70% examples, 166041 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 16:32:28,074 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-10-10 16:32:28,121 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-10 16:32:28,192 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-10 16:32:28,203 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-10 16:32:28,205 : INFO : EPOCH - 3 : training on 374650 raw words (342827 effective words) took 1.8s, 186449 effective words/s\n",
      "2018-10-10 16:32:29,231 : INFO : EPOCH 4 - PROGRESS: at 43.15% examples, 154101 words/s, in_qsize 8, out_qsize 0\n",
      "2018-10-10 16:32:30,039 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-10-10 16:32:30,086 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-10 16:32:30,156 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-10 16:32:30,169 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-10 16:32:30,171 : INFO : EPOCH - 4 : training on 374650 raw words (342995 effective words) took 2.0s, 174957 effective words/s\n",
      "2018-10-10 16:32:31,203 : INFO : EPOCH 5 - PROGRESS: at 45.41% examples, 162430 words/s, in_qsize 8, out_qsize 0\n",
      "2018-10-10 16:32:31,915 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-10-10 16:32:31,934 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-10 16:32:31,990 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-10 16:32:32,008 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-10 16:32:32,009 : INFO : EPOCH - 5 : training on 374650 raw words (342820 effective words) took 1.8s, 187474 effective words/s\n",
      "2018-10-10 16:32:33,033 : INFO : EPOCH 6 - PROGRESS: at 52.15% examples, 181706 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 16:32:33,651 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-10-10 16:32:33,666 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-10 16:32:33,716 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-10 16:32:33,738 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-10 16:32:33,740 : INFO : EPOCH - 6 : training on 374650 raw words (342992 effective words) took 1.7s, 198897 effective words/s\n",
      "2018-10-10 16:32:33,742 : INFO : training on a 2247900 raw words (2057680 effective words) took 11.7s, 176166 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Set values for various parameters\n",
    "num_features = 200    # Word vector dimensionality\n",
    "min_word_count = 3    # Minimum word count\n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 6           # Context window size\n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model\n",
    "W2Vmodel = Word2Vec(sentences=twenty_train_clean_w2v,\n",
    "                    sg=1,\n",
    "                    hs=0,\n",
    "                    workers=num_workers,\n",
    "                    size=num_features,\n",
    "                    min_count=min_word_count,\n",
    "                    window=context,\n",
    "                    sample=downsampling,\n",
    "                    negative=5,\n",
    "                    iter=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w2v_features(w2v_model, words):\n",
    "    \"\"\" Transform a sentence_group (containing multiple lists\n",
    "    of words) into a feature vector. It averages out all the\n",
    "    word vectors of the sentence_group.\n",
    "    \"\"\"\n",
    "    #words = np.concatenate(sentence_group)  # words in text\n",
    "    index2word_set = set(w2v_model.wv.vocab.keys())  # words known to model\n",
    "    \n",
    "    featureVec = np.zeros(w2v_model.vector_size, dtype=\"float32\")\n",
    "    \n",
    "    # Initialize a counter for number of words in a review\n",
    "    nwords = 0\n",
    "    # Loop over each word in the comment and, if it is in the model's vocabulary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            featureVec = np.add(featureVec, w2v_model[word])\n",
    "            nwords += 1.\n",
    "\n",
    "    # Divide the result by the number of words to get the average\n",
    "    if nwords > 0:\n",
    "        featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caperei\\AppData\\Local\\Continuum\\anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel\\__main__.py:16: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "w2v_features_train= list(map(lambda sen_group:\n",
    "                                      get_w2v_features(W2Vmodel, sen_group),\n",
    "                                      np.array(twenty_train_clean_w2v)))\n",
    "\n",
    "w2v_features_test= list(map(lambda sen_group:\n",
    "                                      get_w2v_features(W2Vmodel, sen_group),\n",
    "                                      np.array(twenty_test_clean_w2v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "y_train_w2v = label_encoder.transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caperei\\AppData\\Local\\Continuum\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\caperei\\AppData\\Local\\Continuum\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3856958327416284\n"
     ]
    }
   ],
   "source": [
    "l.fit(w2v_features_train,y_train)\n",
    "y_pred = l.predict_proba(w2v_features_test)\n",
    "print(log_loss(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3897909582067506\n"
     ]
    }
   ],
   "source": [
    "r.fit(w2v_features_train,y_train)\n",
    "y_pred = r.predict_proba(w2v_features_test)\n",
    "print(log_loss(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Embedding Fully Trainned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_general(x):\n",
    "    x=' '.join(word_tokenize(x)) #get rid of '/n'\n",
    "    x=' '.join([i for i in x.split() if '@' not in i])#get rid of emails   \n",
    "    x = re.sub(\"\\d+\", \" \", x)# get rid of numbers   \n",
    "    x = re.sub('[^a-z\\s]', '', x.lower())   # get rid of noise (lower and punctuation)\n",
    "    x = ' '.join([w for w in x.split() if w not in set(stopwords)] )# remove stopwords\n",
    "    wx = ' '.join([porter.stem(w) for w in x.split() ]) # stemming\n",
    "    \n",
    "  \n",
    "    return wx  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_train_ke=map(preprocess_general, twenty_train.data)\n",
    "twenty_train_ke=[x for x in twenty_train_ke]\n",
    "\n",
    "twenty_test_ke=map(preprocess_general, twenty_test.data)\n",
    "twenty_test_ke=[x for x in twenty_test_ke]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14661, 21327, 12555, 16063, 3408, 1496, 19721, 14713, 19974, 10982, 18112, 807, 8023, 11209, 339, 25865, 2286, 2829, 3970, 7400, 21889, 7733, 25915, 25543, 1496, 19185, 15936, 19974, 10982, 123, 12269, 11384, 16124, 1496, 5284, 14713, 9578, 15936, 15307, 18543, 4717, 1891, 19573, 11405, 14589, 12555, 12555, 16063, 19392, 11277, 1145, 18543, 7890, 17123, 11209, 339, 2289, 26857, 3016, 4911, 21659, 3636, 4443]\n"
     ]
    }
   ],
   "source": [
    "# integer encode the documents\n",
    "vocab_size = 27000\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in twenty_train_ke]\n",
    "print(encoded_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14661 21327 12555 16063  3408  1496 19721 14713 19974 10982 18112   807\n",
      "  8023 11209   339 25865  2286  2829  3970  7400 21889  7733 25915 25543\n",
      "  1496 19185 15936 19974 10982   123 12269 11384 16124  1496  5284 14713\n",
      "  9578 15936 15307 18543  4717  1891 19573 11405 14589 12555 12555 16063\n",
      " 19392 11277  1145 18543  7890 17123 11209   339  2289 26857  3016  4911\n",
      " 21659  3636  4443     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "# pad documents to a max length of 4 words\n",
    "max_length = 500\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 100)          2700000   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 50000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 200004    \n",
      "=================================================================\n",
      "Total params: 2,900,004\n",
      "Trainable params: 2,900,004\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "# compile the model\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.000000\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(padded_docs, y_train, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, y_train, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras + GloVe (no train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10967, 10968, 319, 7599, 3, 344, 26, 1500, 7600, 2324, 43, 4323, 5, 640, 15, 2, 72, 13, 31, 37, 318, 454, 14968, 1172, 344, 14969, 57, 7600, 2324, 193, 7, 19, 16, 344, 6702, 1500, 3462, 57, 103, 125, 197, 300, 89, 109, 426, 319, 319, 7599, 1473, 67, 568, 125, 14970, 14971, 640, 15, 1276, 70, 3642, 375, 4063, 298, 8992]\n"
     ]
    }
   ],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts(twenty_train_ke)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "# integer encode the documents\n",
    "encoded_docs = t.texts_to_sequences(twenty_train_ke)\n",
    "#test\n",
    "encoded_docs_test = t.texts_to_sequences(twenty_test_ke)\n",
    "print(encoded_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10967 10968   319  7599     3   344    26  1500  7600  2324    43  4323\n",
      "     5   640    15     2    72    13    31    37   318   454 14968  1172\n",
      "   344 14969    57  7600  2324   193     7    19    16   344  6702  1500\n",
      "  3462    57   103   125   197   300    89   109   426   319   319  7599\n",
      "  1473    67   568   125 14970 14971   640    15  1276    70  3642   375\n",
      "  4063   298  8992     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "# pad documents to a max length of 1000 words\n",
    "max_length = 1000\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "#test\n",
    "padded_docs_test = pad_sequences(encoded_docs_test, maxlen=max_length, padding='post')\n",
    "print(padded_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "#link with data: https://nlp.stanford.edu/projects/glove/\n",
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('\\path\\',encoding=\"utf8\")\n",
    "for line in f:\n",
    "\tvalues = line.split()\n",
    "\tword = values[0]\n",
    "\tcoefs = asarray(values[1:], dtype='float32')\n",
    "\tembeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, i in t.word_index.items():\n",
    "\tembedding_vector = embeddings_index.get(word)\n",
    "\tif embedding_vector is not None:\n",
    "\t\tembedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 1000, 100)         2580700   \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 100000)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 400004    \n",
      "=================================================================\n",
      "Total params: 2,980,704\n",
      "Trainable params: 400,004\n",
      "Non-trainable params: 2,580,700\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=1000, trainable=False)\n",
    "model.add(e)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "# compile the model\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.000000\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(padded_docs, y_train, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, y_train, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras+ GloVe (pre Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.initializers import Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(vocab_size,\n",
    "                            100,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=1000,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 1000, 100)         2580700   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 996, 128)          64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 199, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 195, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 39, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 35, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 1, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 2,825,952\n",
      "Trainable params: 245,252\n",
      "Non-trainable params: 2,580,700\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=1000, trainable=False)\n",
    "model.add(e)\n",
    "\n",
    "conv1=Conv1D(128, 5, activation='relu')\n",
    "model.add(conv1)\n",
    "\n",
    "maxp=MaxPooling1D(5)\n",
    "model.add(maxp)\n",
    "\n",
    "conv2=Conv1D(128, 5, activation='relu')\n",
    "model.add(conv2)\n",
    "\n",
    "maxp2=MaxPooling1D(5)\n",
    "model.add(maxp2)\n",
    "\n",
    "conv3= Conv1D(128, 5, activation='relu')\n",
    "model.add(conv3)\n",
    "\n",
    "maxp3=MaxPooling1D(35)\n",
    "model.add(maxp3)\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "# compile the model\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2257 samples, validate on 1502 samples\n",
      "Epoch 1/2\n",
      "2257/2257 [==============================] - 32s 14ms/step - loss: 1.3875 - acc: 0.3558 - val_loss: 1.0774 - val_acc: 0.5466\n",
      "Epoch 2/2\n",
      "2257/2257 [==============================] - 27s 12ms/step - loss: 0.9856 - acc: 0.5778 - val_loss: 0.8000 - val_acc: 0.5872\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x218adb76f28>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded_docs, y_train, validation_data=(padded_docs_test, y_test),\n",
    "          epochs=2, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:myenv]",
   "language": "python",
   "name": "conda-env-myenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "NLP",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "303.993px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
